---
layout: page
title: Concept Check Solutions
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">
      <ul>
        <li>
          Lecture 2 - Linear Regression (January 29, 2020)
          <ul>
            <li>Question A: Graph 2 is Line 2, Graph 3 is Line 1, Graph 4 is Line 2</li>
            <li>Question B: Graph 2 is large residual, Graph 3 is extreme x value, Graph 4 is pattern in the residuals</li>
          </ul>
        </li>
        <li>
          Lecture 3 - Probabilistic Regression (February 3, 2020)
          <ul>
            <li>Question A: Laplace is $|(\epsilon)|$, Gaussian is $\epsilon^2$, Student T is $\log(1+\epsilon^2)$</li>
            <li>Question B: Laplace is Line 3, Guassian is Line 1, Student T is Line 2</li>
          </ul>
        </li>
        <li>
          Lecture 4 - Linear Classifiation (February 5, 2020)
          <ul>
            <li>Question A: Minimum TPR is 0</li>
            <li>Question B: Maximum TPR is 6/7</li>
          </ul>
        </li>
        <li>
          Lecture 5 - Probabilistic Classifiation (February 10, 2020)
          <ul>
            <li>Question A: Line 2 (if generative), Impossible to gauge without more information (if assumptions unknown)</li>
            <li>Question B: Yes, the unlabeled data helps us here (if generative), although might not always help (if discriminative, for example)</li>
          </ul>
        </li>
        <li>
          Lecture 6 - Model Selection - Frequentist (February 12, 2020)
          <ul>
            <li>Question A: Yes (for A), No probably not (for B)</li>
            <li>Question B: Yes probably (for A), Unclear (for B)</li>
            <li>Question C: No (for A), Yes (for B)</li>
          </ul>
        </li>
        <li>
          Lecture 7 - Model Selection - Bayesian (February 19, 2020)
          <ul>
            <li>Question A: Yes</li>
            <li>Question B: $a_0 = 0$ and $a_2 = 1 - a_1$</li>
            <li>Question C: 0 to 0.5 uniformly</li>
            <li>Question D: 0.5 with probability 1</li>
          </ul>
        </li>
        <li>
          Lecture 8 - Neural Networks 1 (February 24, 2020)
          <ul>
            <li>Question A: Yes (even if one filter)</li>
            <li>Question B: Yes (if assuming multiple filters), No (if assuming only one filter)</li>
            <li>Question C: Yes (even if one filter)</li>
          </ul>
        </li>
        <li>
          Lecture 9 - Neural Networks 2 (February 26, 2020)
          <ul>
            <li>Question A: No</li>
            <li>Question B-i: # Paramaters << # Data - No, won't fit the noise</li>
            <li>Question B-ii: # Paramaters == # Data - Yes, expect roughly 1 perfect model</li>
            <li>Question B-iii: # Paramaters >> # Data - Yes, expect many perfect models</li>
            <li>Question C: SGD implicitly performs regularization</li>
          </ul>
        </li>
        <li>
          Lecture 10 - Support Vector Machine 1 (March 2, 2020)
          <ul>
            <li>NA - Just a discussion this class</li>
          </ul>
        </li>
        <li>
          Lecture 11 - Support Vector Machine 2 (March 2, 2020)
          <ul>
            <li>Question A: Overfit</li>
            <li>Question B: Less of a problem, likely will fit decently well with a smoother boundary</li>
            <li>Question C: Many support points (suggests a wiggly boundary), small margin, cross validation</li>
          </ul>
        </li>
    </div>
</section>
