---
layout: page
title: Concept Check Solutions
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">
      <ul>
        <li>
          Lecture 2 - Linear Regression (January 29, 2020)
          <ul>
            <li>Question A: Graph 2 is Line 2, Graph 3 is Line 1, Graph 4 is Line 2</li>
            <li>Question B: Graph 2 is large residual, Graph 3 is extreme x value, Graph 4 is pattern in the residuals</li>
          </ul>
        </li>
        <li>
          Lecture 3 - Probabilistic Regression (February 3, 2020)
          <ul>
            <li>Question A: Laplace is $|(\epsilon)|$, Gaussian is $\epsilon^2$, Student T is $\log(1+\epsilon^2)$</li>
            <li>Question B: Laplace is Line 3, Guassian is Line 1, Student T is Line 2</li>
          </ul>
        </li>
        <li>
          Lecture 4 - Linear Classifiation (February 5, 2020)
          <ul>
            <li>Question A: Minimum TPR is 0</li>
            <li>Question B: Maximum TPR is 6/7</li>
          </ul>
        </li>
        <li>
          Lecture 5 - Probabilistic Classifiation (February 10, 2020)
          <ul>
            <li>Question A: Line 2 (if generative), Impossible to gauge without more information (if assumptions unknown)</li>
            <li>Question B: Yes, the unlabeled data helps us here (if generative), although might not always help (if discriminative, for example)</li>
          </ul>
        </li>
        <li>
          Lecture 6 - Model Selection - Frequentist (February 12, 2020)
          <ul>
            <li>Question A: Yes (for A), No probably not (for B)</li>
            <li>Question B: Yes probably (for A), Unclear (for B)</li>
            <li>Question C: No (for A), Yes (for B)</li>
          </ul>
        </li>
        <li>
          Lecture 7 - Model Selection - Bayesian (February 19, 2020)
          <ul>
            <li>Question A: Yes</li>
            <li>Question B: $a_0 = 0$ and $a_2 = 1 - a_1$</li>
            <li>Question C: 0 to 0.5 uniformly</li>
            <li>Question D: 0.5 with probability 1</li>
          </ul>
        </li>
        <li>
          Lecture 8 - Neural Networks 1 (February 24, 2020)
          <ul>
            <li>Question A: Yes (even if one filter)</li>
            <li>Question B: Yes (if assuming multiple filters), No (if assuming only one filter)</li>
            <li>Question C: Yes (even if one filter)</li>
          </ul>
        </li>
        <li>
          Lecture 9 - Neural Networks 2 (February 26, 2020)
          <ul>
            <li>Question A: No</li>
            <li>Question B-i: # Paramaters << # Data - No, won't fit the noise</li>
            <li>Question B-ii: # Paramaters == # Data - Yes, expect roughly 1 perfect model</li>
            <li>Question B-iii: # Paramaters >> # Data - Yes, expect many perfect models</li>
            <li>Question C: SGD implicitly performs regularization</li>
          </ul>
        </li>
        <li>
          Lecture 10 - Support Vector Machine 1 (March 2, 2020)
          <ul>
            <li>NA - Just a discussion this class</li>
          </ul>
        </li>
        <li>
          Lecture 11 - Support Vector Machine 2 (March 2, 2020)
          <ul>
            <li>Question A: Overfit</li>
            <li>Question B: Less of a problem, likely will fit decently well with a smoother boundary</li>
            <li>Question C: Many support points (suggests a wiggly boundary), small margin, cross validation</li>
          </ul>
        </li>
        <li>
          Lecture 13 - Clustering (March 23, 2020)
          <ul>
            <li>Question A: First, 3-4 merge and 6-7 merge. Then, 3-4-6-7 merge. Then, 0-3-4-6-7 merge.</li>
            <li>Question B: No</li>
            <li>Question C: Less</li>
          </ul>
        </li>
        <li>
          Lecture 14 - Mixture Models (March 25, 2020)
          <ul>
            <li>Question A: No</li>
            <li>Question B: Image 3</li>
            <li>Question C: Image 1</li>
          </ul>
        </li>
        <li>
          Lecture 15 - Principal Component Analysis (March 30, 2020)
          <ul>
            <li>Question A: x1, x2, x3 (in order from most variance explained to least)</li>
            <li>Question B: No</li>
            <li>Question C: No</li>
          </ul>
        </li>
        <li>
          Lecture 16 - Topic Models (April 1, 2020)
          <ul>
            <li>Question A1: [1/3, 1/3, 1/3], [1/3, 1/3, 1/3], [1/3, 1/3, 1/3]</li>

            Reasoning: First, to recap, a draw from a K-dimesional Dirichlet distribution is a probability vector of length K. If we drew from a Dirichlet and three times we saw [1/3, 1/3, 1/3], that would mean that most of the distribution falls exactly in the middle if you were to visualize this in 3D. This corresponds to the three equally higher numbers in our alpha (think of the beta, which is the Dirichlet with K=2, when both parameters of the beta distribtuion are high and the same, we have the mass in the middle). So, our answer is [1/3, 1/3, 1/3], [1/3, 1/3, 1/3], [1/3, 1/3, 1/3], as it corresponds to the $\alpha_1$ of [10, 10, 10].

            <li>Question A2: [0, 1, 0], [1, 0, 0], [0, 0, 1]</li>

            Reasoning: Since we've drawn three Dirichlet's with all the probability mass on one dimension, this means that distribution is very concentrated in the three corners. This corresponds to three equally lower numberes in our alpha (think of the beta, which is the Dirichlet with K=2, when both parameters of the beta distribtuion are low and the same, we have the mass at the edges). So, our answer is [0, 1, 0], [1, 0, 0], [0, 0, 1], as it corresponds to the $\alpha_2$ of [0.01, 0.01, 0.01].

            <li>Question B1: [13, 11, 10]</li>

            Reasoning: To do this question, since we've seen three data points in the 0th dimension and 1 data point in the 1st dimension, we simply add 3 and 1 to our prior (in the corresponding indices). The prior here was [10, 10, 10], so we have [10+3, 10+1, 10+0]. This is analgous to the Beta story from lecture al ong time ago with the coin flips, where as we saw coin filps, we updated our prior parameters by adding the number of heads to one and the number of tails to the other.

            <li>Question B2: [3.01, 1.01, 0.01]</li>

            Reasoning: To do this question, same logic as the previous question! The prior here was [0.01, 0.01, 0.01], so we have [0.01+3, 0.01+1, 0.01+0].

            <li>Question C: No</li>
          </ul>
        </li>
        <li>
          Lecture 17 - Graphical Models (April 6, 2020)
          <ul>
            <li>Question A:
              <ul>
                <li>Model 1 has 54 (see alternative answer below)</li>
                <li>Model 2 has 5</li>
                <li>Model 3 has 45</li>
                <li>Explanation: <br><br>

                  Currently, for Model 1, we calculate 54 in the following way. We know that they are all discrete variables that can take on 3 values each. Remember that we are not asking for A in the parameter count (although you should still mentally have A in mind). This means B, C, and E each have 9 parameters since each has one parent (so we do 3^2). D has 27 since it has two parents (3^3). In total we hae 54. However, we could techincally argue that with discrete variables that only take on 3 values each, we can really encode all 3 bits of info with just two parameters, since you can extract the third one just by doing 1 minus the sum of the oother two parameters. This is also a valid way to think of the problem. This would lead to B, C, E each having 4 parameters (2^2) and D having 8 parameters (2^3), leading to a total of 16.If this ever comes up on a test or homework, we will make clear what the assumptions are.<br><br>

                  For Model 2, we have B, C, and E each having just 1 parameter, since there is one parent to multiply by (we have a linear relationship). D has 2 parameters, since there are two parents. Leads to a sum of 5. <br><br>

                  For Model 3, we have B, C, and E each having 9 parameters since these all need a 3 by 3 matrix (since it's 3 dimensional now). Model D has 27 parameters (requires a 3 by 6 matrix). This leads to a sum of 45.<br><br>
                </li>
              </ul>
            </li>
            <li>Question B: Only includes linear functions</li>
          </ul>
        </li>
        <li>
          Lecture 18 - Inference in Bayes Nets (April 8, 2020)
          <ul>
            <li>Question A: Cut link from Z to T</li>
            <li>Question B: $\sum_z p(y=1|do(t=1), z=z) p(z=z)$</li>
            <li>Question C: No</li>
          </ul>
        </li>
        <li>
          Lecture 19 - Hidden Markov Models (April 13, 2020)
          <ul>
            <li>Question A: [0, 1, 0]</li>
            <li>Question B: [0, 1/2, 1/2]</li>
          </ul>
        </li>
        <li>
          Lecture 20 - Markov Decision Processes (April 15, 2020)
          <ul>
            <li>Question A: No</li>
            <li>Question B: No</li>
            <li>Question C: Yes</li>
            <li>Question D: No</li>
          </ul>
        </li>
        <li>
          Lecture 21 - Reinforcement Learning (April 20, 2020)
          <ul>
            <li>Question A: Around the top</li>
            <li>Question B: Straight to the right</li>
            <li>Question C: Around the top</li>
            <li>Question D: Straight to the right</li>
          </ul>
        </li>
    </div>
</section>
