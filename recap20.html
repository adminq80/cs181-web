---
layout: page
title: Lecture 20 Recap - Markov Decision Processes
mathjax: true
weight: 0
---

<section class="main-container text">
    <div class="main">

      <h4>Date: April 15, 2020 (<a href="https://harvard.zoom.us/rec/play/6Jcqd-36pz03GYCRswSDC6d6W9W1LvmshnVKqPAKzkyxAngDO1ShYLpEYrdaDcoYCp0WEZy2_vGeKkRA" target="_blank">Lecture Video</a>, <a href="https://piazza.com/class_profile/get_resource/k5fnxwvfh7p7mi/k927rxxtp0j5o5" target="_blank">iPad Notes</a>, <a href="http://bit.ly/cs181cc20" target="_blank">Concept Check</a>, <a href="http://bit.ly/cs181cc20responses" target="_blank">Class Responses</a>, <a href="{{ site.baseurl }}/ccsolutions" target="_blank">Solutions</a>)</h4> 

      <br>

      <h3>Lecture 20 Summary</h3>

      <ul>
        <li><a href="#recap20_1">Intro: Moving from Predictions to Decisions</a></li>
        <li><a href="#recap20_2">Intro: Markov Decision Processes</a></li>
        <li><a href="#recap20_3">How to Solve using Policy Iteration (Method 1)</a></li>
        <li><a href="#recap20_4">Recap and Looking Forward</a></li>
      </ul>

      <h2 id="recap20_1">Intro: Moving from Predictions to Decisions</h2>

      So far in CS181, we've been looking at how to make predictions. Today, we will transition from predictions into the realm of decision making.<br><br>

      To best illustrate this, let's cover a few real world examples. Imagine we're using a map system to go from a starting point to a destination. The map system might have predictive tools that tell us where the traffic is (using realtime data of where cars are right now), and how long different routes take from start to finish. Modeling was used here to figure out all this traffic information and which routes are slower and faster. But now, we need to actually make a decision. Which route do we take? We might have different objectives. Maybe we want the route that is the shortest in expectation. Maybe we're okay with being a little slower on average, if there is a guarantee that we'll be there by some time (say we're trying to make an important interview). These are decisions that don't take place at the prediction time. In order to make these decisions, we'll need to formalize a reward function.<br><br>

      Another real world example is autonomous vehicles. There are tons of predictions that come into place for autonomous vehicles, including where the road is, where the lines on the road are, whether there are signs ahead or not, whether there are pedestrians crossing or not, etc. These are all predictive, and is what we've been doing so far in CS181. Once we have this data, how do we decide what to do next? How should the car actually drive to be safe? Those are the types of things we will start looking at across the next three lectures.

      <h2 id="recap20_2">Intro: Markov Decision Processes</h2>

      First: what is a Markov Decision Process? At the most basic level, it is a framework for modeling decision making (again, remember that we've moved from the world of prediction to the world of decision making). In our model, there is only one agent, and this is the agent that needs to make decisions while interacting with the world (other models incorporate multiple agents, but in CS181 we are only covering situations that have one agent).

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap20_1.png" style="width:50%"  alt="MDP Setup"></img>
      </div>

      <h4>Some Notation, Definitions, and the Goal</h4>

      To explain the setup diagram above, the agent can send actions to the world, and the world will return observations that we call $o$ and rewards that we call $r$.<br><br>

      $o$ represents observations<br><br>
      $r$ represents rewards<br><br>
      $t$ represents the time stamp<br><br>
      $\gamma$ is our discount factor which is between [0, 1) (in the future, rewards are worth less by a factor of $\gamma$ per round of time)<br><br>

      With the above information, we have:<br><br>

      $E [\sum_t \gamma^t r_t]$ represents expected sum of discounted rewards<br><br>

      Goal: agent should choose the actions to maximize the expected sum of discounted rewards.<br><br>

      $h_t = \{a_0, o_0 r_0, a_1, o_1, r_1 ... a_t, o_t, r_t\}$ represents the history of actions, obsevations, and rewards from time 1 to our current time t<br><br>

      State: summarizes a history such that making predictions about the future given the state is equivalent to making those same predictions given the entire history.<br><br>

      <h4>Breaking Down Definition of State</h4>

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap20_2.png" style="width:50%"  alt="Explaining State"></img>
      </div>

      Imagine we had a situation where we wanted to get a person from Start (S) to Goal (G). Assume we have total control of where our person walks. The solid wiggly line to the middle point is our history. What we notice is that if we take the action, "Go Down", and try to guess what happens next, we don't actually need to know anything about the history of where the prson walked from. They could've taken the solid wiggly line or the dotted wiggly line to get to the middle dot, yet all we really needed to know was the location. It's like if someone asked you for directions on the street for how to get to Maxwell Dworkin, you wouldn't ask them, where did you come from? You already have enough info to direct the to MD, since you already know their current locaiton. We say that the location is the state, as it is a sufficient statistic.<br><br>

      Let's say instead of a person that this was a car instead, however, and that the velocity mattered. If we were coming in at 100 miles per hour from the left, moving "Down" would require a huge turn, versus say we were driving in slowly from above, at 10 miles per hour, our action "Down" would be different in each scenario. This means, in this new scenario, simply location is not enough to define the state. We would want to incoporate in the velocity, the acceleration, and essentialy everything we need to be able to predict what happens next. State is whatever is needed to be able to ignore the past. If do we have all the info, then we say the system is Markov in s (state).<br><br>

      Note: this is the official definition of state. Some papers in the literature will sometimes use approximations of states. Say, for example, you might choose to use the frame of a video game as a state, which gives you purely location of all the objects. But to really predict how the objects move, you actually need the velocity of the objects to predict what happens next and to be able to truly call it Markov. It's possible that the paper will just say, we will choose to use just the frame as our approxmiation of state. It's just important to note that that is exactly what it is, an approximation of a state. However, the formal definition of state is still everything you need to know to be able to predict what's next. We will use this definition for state for the next 3 lectures.<br><br>

      <h4>Tying Back and Fully Defining our Markov Decision Process</h4>

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap20_3.png" style="width:50%"  alt="MDP Setup"></img>
      </div>

      Let's come back to the diagram with agent in the world. Now, we're going to assume the world is nice, and that when our agent does some action a, the world will send back the state. You can imagine that the obsevations we had from the situation before get converted via some method into the state (in the real world, this is what would have to happen). Here we're just going to assume we are given the nice state for now. And since the state gives us all the information we need to predict what happen next, this is what makes it a Markov Decision Process (MDP).<br><br>

      Now let's fully define everything:<br><br>

      We have a tuple of information:

      $$\{S, A, R(s, a, s'), T(s' | s, a), \gamma, p_0\}$$

      $S$ are the states<br>
      $A$ are the actions<br>
      $R(s, a, s')$ is the reward for doing $a$ in $s$ and landing in $s'$<br>
      $T(s' | s, a)$ is our transition probabilities, as in where we will likely end up given we start at s and do a (similar idea to the last time when we were doing T(s'|s) in hidden markov models, the only difference is the transition depends on the action, now we incorpoate in the action which determiens the probialities of where we might end up)<br>
      $\gamma$ is the discount factor
      $p_0$ is some start state<br><br>

      Goal: find the best policy. We define a policy to be $\pi$. This could be $\pi(s, a) = P(a|s)$ if this is stochastic, or $\pi(s)=a$ if deterministic. These are both commonly used notations. If stochastic, we have a probability distribution across actions for a given state (what probability to play each action with). If deterministic we simply have one predetermiend action in the policy to play for each given state.<br><br>

      The best policy is as follows:

      $$max_\pi E_{T, p_0} [\sum_t \gamma^t r_t]$$

      Here, $T$ and $p_0$ represent the world, and we're taking the expectation over the world since its random we have no control over it. We do have control over our action however, so we are trying to take the right actions that maximize our returns that we get in the world (by picking the best policy $\pi$).

      <h2 id="recap20_3">How to Solve using Policy Iteration (Method 1)</h2>

      Over the this lecture and next, we will cover three methods of sovling an MDP. Each will have their pros and cons. There's a lot of literature on this topic that includes more complex methods of solving, but overall the three we cover are intuitive and scale relatively well. In addition, the complex methods build off these simpler methods here too, so this is laying down important groundwork.<br><br>

      Now let's begin to break down policy iteration. At a high level, we will propose a policy, and iterate on it to make it better.

      <h4>Policy Evaluation</h4>

      In order to show this though, we first need to cover a simpler problem, policy evaluation. As in, if given a policy, how do we tell how well it'll do in terms of rewards. Formally in the math, given a $\pi$, what is $E[\sum \gamma^t r_t]$? Let's start with some definitions...<br><br>

      Define $V^\pi(s) = E_{\pi, T}[\sum \gamma^t r_t]$<br><br>

      Above, we are defining the value of a policy just by definition. It's the expected sum of discounted rewards we will recieve. Note: the $\pi$ is in the expecation just in case it's a stochastic policy. If it were deterministic, we wouldn't need to do this.<br><br>

      Define $Q^\pi(s, a) = E_{\pi, T} [\sum \gamma^t r_t | s_0=s, a_0=a]$<br><br>

      Here, we define this Q as a slight altercation of the above V. You can think of this as experimenting with a policy. We're essentially saying, just for one instance of time, lets just we did action a instead of following the policy, which lands us in some s'. Then, that's it, thats the extent our experiment! We promise to follow the policy $\pi$ the rest of the way. This is the definition of $Q^\pi(s, a)$, which will be used later.<br><br>

      From this point onwards, let's have our rewards R be deterministic. Remember that our randomness comes from the transition probabilties, which make the rewards different. Note: this is a very reasonable restriciton, because we can always redefine $R(s, a) = E_R[R(s, a)]$. Since this whole R term will be taken over expectation anyways, if we just redefined R to be deterministic, we would be fine.<br><br>

      From this point onwards, let's also have our $\pi$ be a deterministic pi(s). This is a reasonable restriction, because for MDPs, while the proof is out of the scope of the class, it turns out there will always be a determistic optimal policy for a MDP. The general idea is that there is always going to be a determistinic strategy that will exploit the highest possible discounted reward. Note: this won't work in a multiplayer situation though (imagine rock paper scissors; with two players, the optimal $\pi$ has to have some randomesnss, because if you always played scissors the opponent could always play rock and beat you consistently). But because in a MDP there alwys exists a detemristinc strategy $\pi(s)$, so we're going to take advantage of this when we're solving.<br><br>

      <u>Bellman Equation:</u>

      $$V^\pi(s) = R(s, \pi(s))  + \gamma \sum_s' T(s'|s, \pi(s)) V^\pi(s')$$

      Here we introduce the Bellman Equation, which is a recursive equation that gives us what we need in order to solve policy evaluation. Note that there, our policy is deterministic as it is $\pi$ of just s and not of both s and a. Before we go any further, let's first jsutify it mathematically and conceptually.<br><br>

      $$V^\pi(s) = E[\sum_{t=0} \gamma^t r_t | s_0 = s]$$

      $$V^\pi(s) = E[R(s_0, \pi(s_0)) + \sum_{t=1} \gamma^t r_t]$$

      We can extract out the first time step.

      $$V^\pi(s) = R(s_0, \pi(s_0)) + E[\sum_{t=1} \gamma^t r_t]$$

      We can pull out a factor gamma to help us start to recreate V but for s'.

      $$V^\pi(s) = R(s_0, \pi(s_0)) + \gamma E[\sum_{t=1}\gamma^t r_t | s_0 = s_1]$$

      Note that $s_1$ is unknown. It depends on where taking $\pi(s)$ after $s_1$ lands us.

      $$V^\pi(s) = R(s_t, \pi(s_0)) + \gamma E_{s_1 | s_0} [ E [ \sum \gamma^t r_t]]$$

      Since this is dicrete we can use summations to do the expectation. Of course, you can also do this in a continuous domain.

      $$V^\pi(s) = R(s_0, \pi(s_0)) + \gamma \sum_{s_1} T(s_1 | s, a) E_{T, \pi}[\sum \gamma^t r_t | s_0' = s_1]$$

      The right part of this equation is now a redefinition of $V^pi(s')$ so we have completed our recursion.

      $$V^\pi(s) = R(s, \pi(s)) + \gamma \sum T(s' | s, a) V^\pi(s')$$

      The idea here is that the value of being in a particular state is equal to the current reward that I get plus a discounted expectation of what the future value is at the next time step.<br><br>

      <u>So how do we actually solve this to complete Policy Evaluation?</u><br><br>

      Remember, so far we've just written out a recursive equation, and justified why it's correct mathematically and conceptually. But we still haven't solved for V yet.<br><br>

      The good news is, if the state spaces and actions spaces are discrete, it is easy to solve for V. First, we note that the Bellman equation is a linear equation. Across a set of |S| states, we have s unknowns and s equations, with one bellman equation for each state. This means we can simply solve the system of equations.

      <h5>Method 1: Solving for V</h5>

      We can write it out in vector form.

      $$V^\pi = R^\pi + \gamma T^\pi V^\pi$$

      $V^\pi$ is a vector. $R^\pi$ is a vector, where each element is $E_\pi[R(s_t, \pi(s_t))]$ if R is random, and just $R(s_0, \pi(s_0))$ if R is deterministic. $T^\pi(s'|s)$ is $T(s'|s, \pi(s))$.<br><br>

      This gets us to a closed form solution from our linear system of equations:

      $$V^\pi = (I - \pi T^n)^{-1} R^\pi$$

      <h5>Method 2: Solving for V</h5>

      Another way to solve is by first setting $\hat{V}^{\pi(s)}_{k=0} to be anything. This is used more in practice, as this method tends to be more stable. Then we have:

      $$\hat{V}^{\pi(s)}_k$ = $R(s, \pi(s)) + \gamma \sum_s' T(s' | s, \pi(s))V^{\pi(s')}_{k-1}$$

      The $V^{\pi(s')}_{k-1}$ are the old values. With the old values, we can produce the new values. The intuition is that with every iteration, we will push our initialized values (which were just made up) into the discounted future.

      $$\hat{V}^{\pi(s)}_k = R(s, \pi(s)) + \gamma \sum_s' T(s' | s, \pi(s)) [R(s', \pi(s')) + \gamma \sum_s'' T(s'' | s', \pi(s'))V^{\pi(s'')}_{k-2}]$$

      YOu can imagine us plugging in the V back again, and we can see how our original term gets pushed down into the future. If we expand the math out, we end up with a $\gamma^2 \hat{V}^\pi_{k=2}(s'')$, where we notice for each iteration, we add another $\gamma$. Essentially, we have $\gamma^\textrm{iters} V_0^\pi$.<br><br>

      This is a contraction because $\gamma$ is less than 1 (by definition for us), and  eventually this pushes our wrongly initalized value so far into the future that it doesn't matter anymore.

      <h4>Tying Back to Policy Iteration</h4>

      So now, we have two ways to solve for $V^\pi(s)$, our policy evaluation preparation is done. Now, let's look at our policy iteration algorithm, our method of finding the best policy, which was what we wanted in the very first place. As you can see, in it, we use policy evaluation. The algorithm is given below:

      <div class="text-center">
        <img src="{{ site.baseurl }}/images/recap20_4.png" style="width:80%"  alt="Policy Iteration"></img>
      </div>

      The idea is, we will start with some policy, and then use our Q to try some excursion, and if it an action (excursion) is great, we'll actually replace the policy with the better move. While the proof of this is out of the scope of the course, it turns out that the policy improvement theorem proves that this loop will converge to $\pi$*, the optimal policy. Overall, at a high level, you can think of like this: we started with a policy, we tried some excursions, evaluated them, then improved our policy. We then repeated this, iterating until our policy was optimal.<br><br>

      <h2 id="recap20_4">Recap and Looking Forward</h2>

      Where we are so far? So far, we've found one way of getting the optimal policy. We will cover two more methods in the next lecture. In addition, soon we will be covering reinforcement learning (RL). RL is basically when we don't have access to T, and R. What if we had to just interact with the world directly? Much more to come!
    </div>
</section>
